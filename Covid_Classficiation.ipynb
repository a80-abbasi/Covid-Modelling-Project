{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Loading Dataset:"
   ],
   "metadata": {
    "id": "mVnaaG1KooyN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone \"https://github.com/muhammedtalo/COVID-19.git\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3GUv19Mmvzl",
    "outputId": "ef15ac50-6ab1-4f21-f814-92984f066102",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mv \"COVID-19/X-Ray Image DataSet\" \".\""
   ],
   "metadata": {
    "id": "98hjBuBSnQ6k",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9KMTFnwjn_Ey",
    "outputId": "2f5be594-4103-4b19-af56-4a1ecc1d9963",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading pretrained weights:"
   ],
   "metadata": {
    "id": "EZ1Nb8G4or2T",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!git clone \"https://github.com/arnoweng/CheXNet.git\""
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m9xk6jc7ou4T",
    "outputId": "f58c278c-8e1a-436f-8aa4-5b706bef8b93",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mv \"./CheXNet/model.pth.tar\" \".\""
   ],
   "metadata": {
    "id": "XsG5zWdto8HT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!ls"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q1RLr4oXpKCq",
    "outputId": "479ffeb1-3e7c-475b-ce45-ee9f89b1fe88",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "DATASET_ROOT = 'dataset'\n",
    "CKPT_PATH = 'model.pth.tar'"
   ],
   "metadata": {
    "id": "Twjv-sqkpOfK",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Importing from libraries:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "cAOHp7gJmt_p"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f_ywkSuL_bfL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports from my code:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "is-fG1Esmt_v"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-1wbjDZbmt_v",
    "outputId": "1bfb5d14-a002-43f3-e856-2f01f5fb5304"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Setting seed:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "psPtcNwQmt_w"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vsNHPlBBL-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(24)\n",
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5-19Hanbmt_x"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Class for loading pretrained model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "nZNJtgftmt_y"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes, weights=True):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121()\n",
    "        num_features = self.densenet121.classifier.in_features\n",
    "        if weights:\n",
    "            self.densenet121.classifier = nn.Sequential(\n",
    "                nn.Linear(num_features, 14)\n",
    "            )\n",
    "            load_weights(self)\n",
    "        self.densenet121.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def load_weights(model, device='cpu'):\n",
    "    # Code modified from torchvision densenet source for loading from pre .4 densenet weights.\n",
    "    checkpoint = torch.load(CKPT_PATH, map_location=torch.device(device))\n",
    "    state_dict = checkpoint['state_dict']\n",
    "    remove_data_parallel = True  # Change if you don't want to use nn.DataParallel(model)\n",
    "\n",
    "    pattern = re.compile(\n",
    "        r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n",
    "    for key in list(state_dict.keys()):\n",
    "        match = pattern.match(key)\n",
    "        new_key = match.group(1) + match.group(2) if match else key\n",
    "        new_key = new_key[7:] if remove_data_parallel else new_key\n",
    "        state_dict[new_key] = state_dict[key]\n",
    "        # Delete old key only if modified.\n",
    "        if match or remove_data_parallel:\n",
    "            del state_dict[key]\n",
    "\n",
    "    model.load_state_dict(state_dict)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SBRzn70jmt_y"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading pretrained model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "KIQ6CljEmt_z"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = DenseNet121(3).to(device)\n",
    "model"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "C09CteYFmt_0"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataset for loading images:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "uLrd96Axmt_1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, shuffle=True):\n",
    "        if not isinstance(root, Path):\n",
    "            root = Path(root)\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        data = []\n",
    "        for y in os.listdir(self.root):\n",
    "            for x in os.listdir(os.path.join(self.root, y)):\n",
    "                data.append({'image': x, 'label': y})\n",
    "        self.dataframe = pd.DataFrame(data)\n",
    "        self.label_names, labels = np.unique(self.dataframe['label'], return_inverse=True)\n",
    "        self.dataframe['label'] = labels\n",
    "        self.label_counts = self.dataframe['label'].value_counts()\n",
    "        self.label_weights = len(self.dataframe) / self.label_counts\n",
    "        self.label_weights = self.label_weights / self.label_weights.sum()\n",
    "        if shuffle:\n",
    "            self.dataframe = self.dataframe.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.dataframe.loc[index]\n",
    "        image_name, label = item['image'], item['label']\n",
    "        label_name = self.label_names[label]\n",
    "        image_path = self.root / label_name / image_name\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "mWbxLB9Dmt_1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "normalize = T.Normalize([0.485, 0.456, 0.406],\n",
    "                        [0.229, 0.224, 0.225])\n",
    "\n",
    "# dataset = CovidDataset(DATASET_ROOT, transform=T.Compose([\n",
    "#                                     T.Resize(256),\n",
    "#                                     T.TenCrop(224),\n",
    "#                                     T.Lambda\n",
    "#                                     (lambda crops: torch.stack([T.ToTensor()(crop) for crop in crops])),\n",
    "#                                     T.Lambda\n",
    "#                                     (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "#                                 ]))\n",
    "dataset = CovidDataset(DATASET_ROOT,\n",
    "                       transform=T.Compose([\n",
    "                           T.Resize((256, 256)),\n",
    "                           T.ToTensor(),\n",
    "                           normalize\n",
    "                       ]))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "uYtL_1CEmt_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset.label_weights"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset[0][0].shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VNXvOCger48r",
    "outputId": "4cf2dfab-2b58-4864-f990-b26426c1f001",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model(dataset[0][0].to(device).unsqueeze(0))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iZOzxny9suKL",
    "outputId": "e1ec85a7-3883-40f2-f3c5-4f2a94f964ca",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dataloaders:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "aDn08H9Nmt_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# splitting train and test sets\n",
    "voc_len = len(dataset)\n",
    "train_len = int(0.8 * voc_len)\n",
    "test_len = voc_len - train_len\n",
    "train_set, test_set = random_split(dataset, [train_len, test_len])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "xcsDe4m2mt_2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# splitting train and val sets\n",
    "train_len = int(0.8 * len(train_set))\n",
    "val_len = len(train_set) - train_len\n",
    "train_set, val_set = random_split(train_set, [train_len, val_len])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, 64, shuffle=True)\n",
    "val_loader = DataLoader(val_set, 64, shuffle=True)\n",
    "test_loader = DataLoader(test_set, 64, shuffle=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "oXW8x8TJmt_2"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training functions:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m3l4cK-_KuvG",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import time, tqdm\n",
    "\n",
    "\n",
    "def train(model, train_loader, criterion, optimizer, epoch):\n",
    "    train_loss = 0\n",
    "    N_train = len(train_loader.dataset)\n",
    "\n",
    "    model.train()\n",
    "    with tqdm.tqdm(enumerate(train_loader), total=len(train_loader)) as pbar:\n",
    "        for i, (x, y) in pbar:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            p = model(x)\n",
    "\n",
    "            loss = criterion(p, y)\n",
    "            train_loss += loss.item() * len(x)\n",
    "\n",
    "            pbar.set_description(f'Epoch:{epoch}, Train Loss: {train_loss / N_train:.3e}')\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    train_loss /= N_train\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, epoch=0, metrics=None):\n",
    "    val_loss = 0\n",
    "    N_val = len(val_loader.dataset)\n",
    "    Y = []\n",
    "    Y_pred = []\n",
    "    model.eval()\n",
    "    with torch.no_grad(), tqdm.tqdm(enumerate(val_loader), total=len(val_loader)) as pbar:\n",
    "        for i, (x, y) in pbar:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            p = model(x)\n",
    "            y_pred = p.argmax(dim=-1)\n",
    "            loss = criterion(p, y)\n",
    "            val_loss += loss.item() * len(x)\n",
    "\n",
    "            pbar.set_description(f'Epoch:{epoch}, Val Loss: {val_loss / N_val:.3e}')\n",
    "            Y.append(y.cpu().numpy())\n",
    "            Y_pred.append(y_pred.cpu().numpy())\n",
    "    print('-------------------------------------------------------------------')\n",
    "    Y = np.concatenate(Y)\n",
    "    Y_pred = np.concatenate(Y_pred)\n",
    "    val_loss /= N_val\n",
    "    if metrics is not None:\n",
    "        return {metric: metric_func(Y, Y_pred) for metric, metric_func in metrics.items()}\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFdNnS7Zk1oy",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = list(), list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmeKLMBIMsSF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, dataloaders, optimizer, num_epochs, model_name='pytorch_model'):\n",
    "    val = len(dataloaders) == 2\n",
    "    if val:\n",
    "        train_loader, val_loader = dataloaders\n",
    "    else:\n",
    "        train_loader, = dataloaders\n",
    "\n",
    "    val_loss_min = np.inf\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer, epoch)\n",
    "        train_losses.append(train_loss)\n",
    "        if val:\n",
    "            val_loss = validate(model, val_loader, criterion, epoch)\n",
    "            val_losses.append(val_loss)\n",
    "\n",
    "            if val_loss <= val_loss_min:\n",
    "                torch.save(model.state_dict(), f'{model_name}.pt')\n",
    "                val_loss_min = val_loss\n",
    "    if val:\n",
    "        # load best model during different epochs\n",
    "        model.load_state_dict(torch.load(f'{model_name}.pt'))\n",
    "        plt.plot(val_losses, label='val')\n",
    "\n",
    "    plt.plot(train_losses, label='train')\n",
    "    plt.legend()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Freezing pretrained layers:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "for param in model.densenet121.classifier.parameters():\n",
    "    param.requires_grad = True"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "nwDNVJS4mt_4"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Learning Config:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor(dataset.label_weights.sort_index().tolist())).to(device)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "hDpzasmdmt_4",
    "outputId": "f16021fc-e8a9-4d97-fa51-cf040fe9045c"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Overfitting on a small dataset:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "small_len = int(0.1 * len(train_set))\n",
    "print(small_len)\n",
    "other_len = len(train_set) - small_len\n",
    "_, small_set = random_split(train_set, [other_len, small_len])\n",
    "small_loader = DataLoader(small_set, 64, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model(model, criterion, [small_loader], optimizer, 30);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tuning hyper-parameters:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# todo!"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training model:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_model(model, criterion, [train_loader, val_loader], optimizer, 30, 'covid-classification');"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "average_policy = 'macro'\n",
    "metrics = {'accuracy': accuracy_score, 'precision': lambda y1, y2: precision_score(y1, y2, average=average_policy),\n",
    "           'recall': lambda y1, y2: recall_score(y1, y2, average=average_policy),\n",
    "           'f1': lambda y1, y2: f1_score(y1, y2, average=average_policy)}"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgIAUjEitbJG",
    "outputId": "31071aa4-6b0e-47e1-a906-72d813fb8c87",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "validate(model, test_loader, criterion, metrics=metrics)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZztlzk4yEn7",
    "outputId": "da58c47e-2b02-4028-9e5c-797e224b88c9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}